{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff8a741",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a423b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import mne\n",
    "from src.MovieEEGSourcePipeline.source import _load_epochs, make_forward, make_inverse_from_baseline\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"data/epochs\")\n",
    "SUBJECTS_DIR = Path(\"data\")\n",
    "FS_SUBJECT = \"fsaverage\"\n",
    "\n",
    "# Use an ico-4 source space (coarse; appropriate for Yeo-7 parcellation)\n",
    "FS_SRC_FNAME = SUBJECTS_DIR / FS_SUBJECT / \"bem\" / \"fsaverage-ico-4-src.fif\"\n",
    "FS_BEM_FNAME = SUBJECTS_DIR / FS_SUBJECT / \"bem\" / \"fsaverage-5120-5120-5120-bem-sol.fif\"\n",
    "\n",
    "\n",
    "def extract_stcs(\n",
    "    epochs: mne.Epochs,\n",
    "    inv: mne.minimum_norm.InverseOperator,\n",
    ") -> list[mne.SourceEstimate]:\n",
    "    stcs = mne.minimum_norm.apply_inverse_epochs(\n",
    "        epochs,\n",
    "        inverse_operator=inv,\n",
    "        method=\"eLORETA\",\n",
    "        lambda2=1.0 / 9.0,\n",
    "        pick_ori=\"normal\",     # explicit orientation choice\n",
    "        return_generator=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return stcs\n",
    "\n",
    "\n",
    "def average_stcs(stcs: list[mne.SourceEstimate]) -> mne.SourceEstimate:\n",
    "    if len(stcs) == 0:\n",
    "        raise ValueError(\"No STCs to average.\")\n",
    "    # Average across epochs (time x vertices)\n",
    "    stcs_avg = np.mean([stc.data for stc in stcs], axis=0)\n",
    "\n",
    "    stc_mean = mne.SourceEstimate(\n",
    "        stcs_avg,\n",
    "        vertices=stcs[0].vertices,\n",
    "        tmin=stcs[0].tmin,\n",
    "        tstep=stcs[0].tstep,\n",
    "        subject=stcs[0].subject,\n",
    "    )\n",
    "    return stc_mean\n",
    "\n",
    "\n",
    "# Helpers\n",
    "def run_source_localisation(data_dir, fs_subject, fs_src_fname, fs_bem_fname):\n",
    "\n",
    "    # Pick a single example file to define channel set for forward model\n",
    "    example = next(data_dir.glob(\"*_city_l_epo.fif\"), None)\n",
    "    if example is None:\n",
    "        example = next(data_dir.glob(\"*_epo.fif\"), None)\n",
    "    if example is None:\n",
    "        raise FileNotFoundError(f\"No epoch files found in {data_dir} to build forward model.\")\n",
    "    fwd = make_forward(example, FS_SUBJECT=fs_subject, FS_SRC_FNAME=fs_src_fname, FS_BEM_FNAME=fs_bem_fname)\n",
    "\n",
    "    inv_cache = {}  # subject -> inverse operator built from baseline1\n",
    "\n",
    "    for epochs_path in sorted(data_dir.glob(\"*_epo.fif\")):\n",
    "        m = re.search(r\"^(\\d+)_([^_]+_[^_]+)_epo$\", epochs_path.stem)\n",
    "        if m is None:\n",
    "            continue\n",
    "        subject, film = m.groups()\n",
    "\n",
    "        epochs = _load_epochs(epochs_path)\n",
    "\n",
    "        out_dir = Path(\"data/stcs\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_path_stcs = out_dir / f\"{subject}_{film}_stcs.npz\"\n",
    "\n",
    "        if output_path_stcs.exists():\n",
    "            print(f\"STCs for {subject} {film} already exist, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure we have an inverse per subject (from baseline1)\n",
    "        if subject not in inv_cache:\n",
    "            # pick baseline portion (and avoid immediate pre-cut because of anticipatory activity)\n",
    "            epochs_base = epochs.copy().crop(tmin=-0.2, tmax=-0.05)\n",
    "            inv_cache[subject] = make_inverse_from_baseline(epochs_base, fwd)\n",
    "\n",
    "        inv = inv_cache[subject]\n",
    "\n",
    "        print(f\">>>>>>>> {subject} {film}\")\n",
    "        stcs = extract_stcs(epochs, inv)\n",
    "        stcs_avg = average_stcs(stcs)\n",
    "        # save the averaged STC data in compressed format\n",
    "        np.savez_compressed(\n",
    "            output_path_stcs,\n",
    "            data=stcs_avg.data,\n",
    "            vertices=stcs_avg.vertices,\n",
    "            tmin=stcs_avg.tmin,\n",
    "            tstep=stcs_avg.tstep,\n",
    "            subject=stcs_avg.subject,\n",
    "        )\n",
    "\n",
    "# open stcs files and average across subjects for each film, then save the averaged stc data\n",
    "\n",
    "def load_all_stcs(stcs_dir=Path(\"data/stcs\")):\n",
    "    '''\n",
    "    return scambled and linear\n",
    "    '''\n",
    "    cond_A_suffixes = (\"city_nl\", \"art_nl\")\n",
    "    cond_B_suffixes = (\"city_l\",  \"art_l\")\n",
    "\n",
    "    evoked_A = []\n",
    "    evoked_B = []\n",
    "\n",
    "    # collect subjects from filenames\n",
    "    subjects = sorted({p.name.split(\"_\")[0] for p in stcs_dir.glob(\"*_stcs.npz\")})\n",
    "\n",
    "    meta = None\n",
    "\n",
    "    def _assert_same_meta(meta_ref, meta_new, label):\n",
    "        if not all(np.array_equal(v, v0) for v, v0 in zip(meta_new[\"vertices\"], meta_ref[\"vertices\"])):\n",
    "            raise ValueError(f\"Vertices mismatch for {label}.\")\n",
    "        if meta_new[\"tmin\"] != meta_ref[\"tmin\"]:\n",
    "            raise ValueError(f\"tmin mismatch for {label}: {meta_new['tmin']} != {meta_ref['tmin']}\")\n",
    "        if meta_new[\"tstep\"] != meta_ref[\"tstep\"]:\n",
    "            raise ValueError(f\"tstep mismatch for {label}: {meta_new['tstep']} != {meta_ref['tstep']}\")\n",
    "\n",
    "    for sub in subjects:\n",
    "        # scrambled\n",
    "        files_A = [\n",
    "            stcs_dir / f\"{sub}_{c}_stcs.npz\"\n",
    "            for c in cond_A_suffixes\n",
    "            if (stcs_dir / f\"{sub}_{c}_stcs.npz\").exists()\n",
    "        ]\n",
    "\n",
    "        # linear\n",
    "        files_B = [\n",
    "            stcs_dir / f\"{sub}_{c}_stcs.npz\"\n",
    "            for c in cond_B_suffixes\n",
    "            if (stcs_dir / f\"{sub}_{c}_stcs.npz\").exists()\n",
    "        ]\n",
    "\n",
    "        if not files_A or not files_B:\n",
    "            print('skip incomplete subjects safely')\n",
    "            continue\n",
    "\n",
    "        # average within subject across films for each condition\n",
    "        sub_A = []\n",
    "        sub_B = []\n",
    "\n",
    "        for fp in files_A:\n",
    "            npz = np.load(fp, allow_pickle=True)\n",
    "            sub_A.append(npz[\"data\"])\n",
    "            meta_new = {\n",
    "                \"vertices\": npz[\"vertices\"],\n",
    "                \"tmin\": float(npz[\"tmin\"]),\n",
    "                \"tstep\": float(npz[\"tstep\"]),\n",
    "                \"subject\": str(npz[\"subject\"])\n",
    "            }\n",
    "            if meta is None:\n",
    "                meta = meta_new\n",
    "            else:\n",
    "                _assert_same_meta(meta, meta_new, f\"{fp.name}\")\n",
    "        for fp in files_B:\n",
    "            npz = np.load(fp, allow_pickle=True)\n",
    "            sub_B.append(npz[\"data\"])\n",
    "            meta_new = {\n",
    "                \"vertices\": npz[\"vertices\"],\n",
    "                \"tmin\": float(npz[\"tmin\"]),\n",
    "                \"tstep\": float(npz[\"tstep\"]),\n",
    "                \"subject\": str(npz[\"subject\"])\n",
    "            }\n",
    "            if meta is None:\n",
    "                meta = meta_new\n",
    "            else:\n",
    "                _assert_same_meta(meta, meta_new, f\"{fp.name}\")\n",
    "\n",
    "        evoked_A.append(np.mean(sub_A, axis=0))\n",
    "        evoked_B.append(np.mean(sub_B, axis=0))\n",
    "\n",
    "    evoked_A = np.stack(evoked_A)\n",
    "    evoked_B = np.stack(evoked_B)\n",
    "    return evoked_A, evoked_B, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_source_localisation(\n",
    "        data_dir=DATA_DIR,\n",
    "        fs_subject=FS_SUBJECT,\n",
    "        fs_src_fname=FS_SRC_FNAME,\n",
    "        fs_bem_fname=FS_BEM_FNAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9fa9ae",
   "metadata": {},
   "source": [
    "## Average STC data across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdb4144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_A, evoked_B, meta = load_all_stcs()\n",
    "\n",
    "GA_A = evoked_A.mean(0)\n",
    "GA_B = evoked_B.mean(0)\n",
    "\n",
    "if meta is None:\n",
    "    raise RuntimeError(\"No subjects with complete conditions found.\")\n",
    "\n",
    "GA_A_stc = mne.SourceEstimate(\n",
    "    GA_A,\n",
    "    vertices=meta[\"vertices\"].tolist(),\n",
    "    tmin=meta[\"tmin\"],\n",
    "    tstep=meta[\"tstep\"],\n",
    "    subject=meta[\"subject\"],\n",
    ")\n",
    "\n",
    "GA_B_stc = mne.SourceEstimate(\n",
    "    GA_B,\n",
    "    vertices=meta[\"vertices\"].tolist(),\n",
    "    tmin=meta[\"tmin\"],\n",
    "    tstep=meta[\"tstep\"],\n",
    "    subject=meta[\"subject\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f108a7",
   "metadata": {},
   "source": [
    "## Visualusation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in [-0.2, 0.0, 0.1, 0.2, 0.4, 0.8]:\n",
    "t = 0.5\n",
    "stc_one = GA_A_stc.copy().crop(t, t)\n",
    "\n",
    "brain = stc_one.plot(\n",
    "    subject=\"fsaverage\",\n",
    "    subjects_dir=SUBJECTS_DIR,\n",
    "    hemi=\"both\",\n",
    "    views=\"lateral\",\n",
    "    colorbar=True,\n",
    "    time_viewer=True,\n",
    "    backend=\"notebook\" # for faster html saving\n",
    "    )\n",
    "\n",
    "plotter = brain._renderer.plotter\n",
    "\n",
    "out_dir = Path(\"data/stc_vis\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_html = out_dir / f\"scrambled_{int(t*1000)}.html\"\n",
    "\n",
    "if hasattr(brain, \"save_html\"):\n",
    "    brain.save_html(out_html, time_viewer=True)\n",
    "else:\n",
    "    plotter.export_html(str(out_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7c8f6",
   "metadata": {},
   "source": [
    "## compare STCs between conditions across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bf2be75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading labels from parcellation...\n",
      "   read 35 labels from /Users/yeganeh/Codes/MovieEEG-SourcePipeline/data/fsaverage/label/lh.aparc.annot\n",
      "   read 34 labels from /Users/yeganeh/Codes/MovieEEG-SourcePipeline/data/fsaverage/label/rh.aparc.annot\n"
     ]
    }
   ],
   "source": [
    "# extract posterior labels and combine them\n",
    "labels = mne.read_labels_from_annot(\n",
    "    subject=\"fsaverage\",\n",
    "    parc=\"aparc\",  # Desikan-Killiany\n",
    "    subjects_dir=SUBJECTS_DIR\n",
    ")\n",
    "\n",
    "# occipital + cuneus + precuneus\n",
    "posterior_labels = [\n",
    "    label for label in labels\n",
    "    if (\"occipital\" in label.name.lower()) or\n",
    "    #    (\"cuneus\" in label.name.lower()) or\n",
    "       (\"precuneus\" in label.name.lower())\n",
    "]\n",
    "\n",
    "if len(posterior_labels) == 0:\n",
    "    raise RuntimeError(\"No posterior labels matched the selection rule.\")\n",
    "\n",
    "posterior_label = posterior_labels[0]\n",
    "for lab in posterior_labels[1:]:\n",
    "    posterior_label += lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f01968fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stc_lin and stc_scr are subject-level averaged STCs\n",
    "def create_sub_stcs(data, meta):\n",
    "    return mne.SourceEstimate(\n",
    "        data,\n",
    "        vertices=meta[\"vertices\"].tolist(),\n",
    "        tmin=meta[\"tmin\"],\n",
    "        tstep=meta[\"tstep\"],\n",
    "        subject=meta[\"subject\"]\n",
    "        )\n",
    "\n",
    "scrambled, linear, meta = load_all_stcs()\n",
    "n_sub = scrambled.shape[0]\n",
    "subject_results = []\n",
    "times_of_interest = [0.3, 0.5, 0.7]\n",
    "window = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "97d1842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in range(n_sub):\n",
    "\n",
    "    stc_lin = create_sub_stcs(linear[sub], meta)\n",
    "    stc_scr = create_sub_stcs(scrambled[sub], meta)\n",
    "\n",
    "    stc_lin_post = stc_lin.in_label(posterior_label)\n",
    "    stc_scr_post = stc_scr.in_label(posterior_label)\n",
    "\n",
    "    for t in times_of_interest:\n",
    "        tmin = t - window\n",
    "        tmax = t + window\n",
    "\n",
    "        idx_min = stc_lin.time_as_index(tmin)[0]\n",
    "        idx_max = stc_lin.time_as_index(tmax)[0]\n",
    "\n",
    "        # Include tmax in the averaging window and avoid out-of-bounds slicing.\n",
    "        idx_max = min(idx_max + 1, stc_lin_post.data.shape[1])\n",
    "        if idx_min >= idx_max:\n",
    "            raise RuntimeError(f\"Empty time window for t={t}s (idx_min={idx_min}, idx_max={idx_max}).\")\n",
    "\n",
    "        lin_data = stc_lin_post.data[:, idx_min:idx_max]\n",
    "        scr_data = stc_scr_post.data[:, idx_min:idx_max]\n",
    "\n",
    "        # RMS across posterior vertices and window samples.\n",
    "\n",
    "        # lin_rms = np.var(lin_data)\n",
    "        # scr_rms = np.var(scr_data)\n",
    "        lin_rms = np.sqrt(np.mean(lin_data ** 2))\n",
    "        scr_rms = np.sqrt(np.mean(scr_data ** 2))\n",
    "\n",
    "        subject_results.append({\n",
    "            \"subject\": sub,\n",
    "            \"time\": t,\n",
    "            \"linear_rms\": lin_rms,\n",
    "            \"scrambled_rms\": scr_rms,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "422de2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 ms (n=306): t=-0.777, p=0.4379, p_bonf=1.0000, cohen_d=-0.044\n",
      "500 ms (n=306): t=-1.164, p=0.2455, p_bonf=0.7366, cohen_d=-0.067\n",
      "700 ms (n=306): t=-0.935, p=0.3507, p_bonf=1.0000, cohen_d=-0.053\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(subject_results)\n",
    "\n",
    "results = []\n",
    "for t in times_of_interest:\n",
    "    sub_df = (\n",
    "        df[df[\"time\"] == t]\n",
    "        .sort_values(\"subject\")\n",
    "        .dropna(subset=[\"linear_rms\", \"scrambled_rms\"])\n",
    "    )\n",
    "\n",
    "    tstat, pval = ttest_rel(\n",
    "        sub_df[\"linear_rms\"].to_numpy(),\n",
    "        sub_df[\"scrambled_rms\"].to_numpy(),\n",
    "    )\n",
    "    diff = sub_df[\"linear_rms\"] - sub_df[\"scrambled_rms\"]\n",
    "    d = (diff.mean()) / diff.std(ddof=1)\n",
    "\n",
    "    results.append({\"time\": t, \"n\": len(sub_df), \"t\": tstat, \"p\": pval, \"d\": d})\n",
    "\n",
    "# Bonferroni correction across the 3 planned timepoint tests.\n",
    "m = len(results)\n",
    "for r in results:\n",
    "    r[\"p_bonf\"] = min(r[\"p\"] * m, 1.0)\n",
    "\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"{int(r['time']*1000)} ms (n={r['n']}): \"\n",
    "        f\"t={r['t']:.3f}, p={r['p']:.4f}, p_bonf={r['p_bonf']:.4f}\"\n",
    "        f\", cohen_d={r['d']:.3f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pixi(MovieEEG)",
   "language": "python",
   "name": "movieeeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
